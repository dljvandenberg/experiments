{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Wikipedia pages using geosearch\n",
    "\n",
    "This notebook uses the Wikipedia API to do a geosearch (based on longitude, latitude, radius) for matching Wikipedia articles, and then retrieves the intro texts of each of the matched articles. Results are exported to a csv file and visualized on a map.\n",
    "\n",
    "See MediaWiki API: https://www.mediawiki.org/wiki/Extension:GeoData\n",
    "\n",
    "Written by Dennis van den Berg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:28:39.475116Z",
     "start_time": "2019-05-21T00:28:38.726737Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium import plugins\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T19:12:16.550057Z",
     "start_time": "2019-05-20T19:12:16.544428Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:28:39.520195Z",
     "start_time": "2019-05-21T00:28:39.477305Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def chunks(list, chunksize):\n",
    "    \"\"\"Yield successive chunks from list.\"\"\"\n",
    "    for chunk_number in range(0, len(list), chunksize):\n",
    "        yield list[chunk_number : chunk_number + chunksize]\n",
    "\n",
    "\n",
    "def wikipedia_get_intro_texts(page_names, query_size=10):\n",
    "    \"\"\"Get intro texts for list of pages\"\"\"\n",
    "    \n",
    "    # Create empty dataframe for intro texts\n",
    "    df_intro_texts_all = pd.DataFrame()\n",
    "    \n",
    "    # Split list of page_names into chunks of length query_size and do multiple queries\n",
    "    for page_names_chunk in chunks(list=page_names, chunksize=query_size):\n",
    "    \n",
    "        # Wikipedia intro text query\n",
    "        page_names_string = '|'.join(page_names_chunk)\n",
    "        url = \"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles={}\" \\\n",
    "            .format(page_names_string)\n",
    "\n",
    "        # API call\n",
    "        intro_texts = requests.get(url)\n",
    "\n",
    "        # Unpack json and convert to dataframe\n",
    "        df_intro_texts = pd.DataFrame(intro_texts.json()['query']['pages']).transpose()\n",
    "        \n",
    "        # Append results\n",
    "        df_intro_texts_all = df_intro_texts_all.append(df_intro_texts)\n",
    "    \n",
    "    # Cleaning: remove newlines in 'extract' column\n",
    "    df_intro_texts_all['extract'] = df_intro_texts_all['extract'].str.replace('\\n', ' ').replace('\\r', '')\n",
    "    \n",
    "    return(df_intro_texts_all)\n",
    "\n",
    "\n",
    "def wikipedia_geosearch(lat, lon, radius_meters = 10000, max_results = 500, output_file=None):\n",
    "    \"\"\"Get wikipedia matches by lat/lon coordinates\"\"\"\n",
    "    \n",
    "    # Wikipedia geosearch query\n",
    "    url = \"https://en.wikipedia.org/w/api.php?action=query&list=geosearch&format=json&gslimit={}&gsradius={}&gscoord={}|{}\" \\\n",
    "        .format(str(max_results), str(radius_meters), str(lat), str(lon))\n",
    "\n",
    "    # API call for pages matching location\n",
    "    geo_results = requests.get(url)\n",
    "\n",
    "    # Unpack json and convert to dataframe\n",
    "    df_geo_results = pd.DataFrame(geo_results.json()['query']['geosearch'])\n",
    "    \n",
    "    # API call for intro texts of pages\n",
    "    page_names = list(df_geo_results['title'])\n",
    "    df_intro_texts = wikipedia_get_intro_texts(page_names)\n",
    "    \n",
    "    # Merge intro_texts into geo_results\n",
    "    df_geo_results = df_geo_results.merge(df_intro_texts, how='left')\n",
    "    \n",
    "    # Write to file if needed\n",
    "    if(output_file!=None):\n",
    "        print(\"Results saved to file:\", output_file)\n",
    "        df_geo_results.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    \n",
    "    return(df_geo_results)\n",
    "\n",
    "\n",
    "def visualize_map(df):\n",
    "    \"\"\"Visualize lat/lon locations on map using Folium\"\"\"\n",
    "    \n",
    "    # Start lon/lat\n",
    "    latitude = df.iloc[0].loc[['lat']]\n",
    "    longitude = df.iloc[0].loc[['lon']]\n",
    "    \n",
    "    # Initialize map\n",
    "    map = folium.Map(location = [latitude, longitude], zoom_start = 12)\n",
    " \n",
    "    # Instantiate a mark cluster object for the locations in the dataframe\n",
    "    locations = plugins.MarkerCluster().add_to(map)\n",
    " \n",
    "    # Loop through the dataframe and add each data point to the mark cluster\n",
    "    for lat, lon, label, in zip(df.lat, df.lon, df.title):\n",
    "        popup_string = '<a href=\" {} \"target=\"_blank\"> {} </a>'.format('https://en.wikipedia.org/wiki/'+label, label)\n",
    "        \n",
    "        folium.Marker(\n",
    "            location=[lat, lon],\n",
    "            icon=None,\n",
    "            popup = folium.Popup(popup_string),\n",
    "        ).add_to(locations)\n",
    "\n",
    "    # Display map\n",
    "    return(map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:28:44.887838Z",
     "start_time": "2019-05-21T00:28:39.522328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Search wikipedia by location\n",
    "df_wiki_locations = wikipedia_geosearch(lat = 52.0894444, \n",
    "                                 lon = 5.1077981, \n",
    "                                 max_results = 200, \n",
    "                                 output_file = \"wikipedia_utrecht.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:28:44.907406Z",
     "start_time": "2019-05-21T00:28:44.889329Z"
    }
   },
   "outputs": [],
   "source": [
    "df_wiki_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:28:45.073832Z",
     "start_time": "2019-05-21T00:28:44.910114Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_map(df_wiki_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TO DO: retrieve images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:28:45.285152Z",
     "start_time": "2019-05-21T00:28:45.075991Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "params = {\n",
    "    'action':'query',\n",
    "    'format':'json',\n",
    "    'prop':'images',\n",
    "    'titles':'Lunetten|Utrecht Centraal railway station'\n",
    "}\n",
    "\n",
    "request = session.get(url=url, params=params)\n",
    "\n",
    "data = request.json()\n",
    "results = pd.DataFrame(data['query']['pages']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:28:45.292244Z",
     "start_time": "2019-05-21T00:28:45.287721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['query']['pages']['3044682']['images'][2]['title']"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
